{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T15:41:59.276208Z",
     "start_time": "2024-04-30T15:41:59.257189Z"
    }
   },
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import utils\n",
    "from A2C import ActorCritic\n",
    "\n",
    "\n",
    "def data_collection(state: np.array, nb_steps: int, env: gym.Env, actor_critic: ActorCritic, gamma: float):\n",
    "    discounted_returns = 0.0\n",
    "    step_state = state\n",
    "    actions = []\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    step = 0\n",
    "    while step < nb_steps and not truncated:\n",
    "        # Compute action\n",
    "        action = actor_critic.sample_action(step_state)\n",
    "        actions.append(action)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        discounted_returns += gamma ** step * float(reward)\n",
    "        step_state = torch.Tensor(next_state)\n",
    "        step += 1\n",
    "\n",
    "    discounted_returns += gamma ** step * (1 - terminated) * actor_critic.get_value(step_state)\n",
    "\n",
    "    return actions[0], step_state, discounted_returns, total_reward, terminated or truncated\n",
    "\n",
    "\n",
    "def multistep_advantage_actor_critic_episode(\n",
    "        env: gym.Env,\n",
    "        actor_critic: ActorCritic,\n",
    "        iteration: mp.Value,\n",
    "        gamma: float,\n",
    "        lock: mp.Lock,\n",
    "        nb_steps: int = 1,\n",
    "        max_iter: int = 500000\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Run an episode of multistep A2C on the given environment.\n",
    "    :return: tuple containing the (total) reward for the episode\n",
    "    \"\"\"\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = torch.Tensor(state)\n",
    "\n",
    "    debug_infos_interval = 1000\n",
    "    evaluate_interval = 20000\n",
    "\n",
    "    while iteration.value <= max_iter and not done:\n",
    "        print(iteration.value)\n",
    "        action, next_state, discounted_returns, rewards, done = data_collection(\n",
    "            state,\n",
    "            nb_steps,\n",
    "            env,\n",
    "            actor_critic.copy(),\n",
    "            gamma\n",
    "        )\n",
    "        total_reward += rewards\n",
    "\n",
    "        lock.acquire()\n",
    "        try:\n",
    "            actor_loss, critic_loss = actor_critic.update(discounted_returns, state, action)\n",
    "\n",
    "            if iteration.value % debug_infos_interval == 0:\n",
    "                print(f\"\\nIteration {iteration.value}: \\n\\tActor loss = {actor_loss} \\n\\tCritic loss = {critic_loss}\")\n",
    "\n",
    "            if iteration.value % evaluate_interval == 0:\n",
    "                avg_return = evaluate(actor_critic)\n",
    "                print(f\"\\nEvaluation at iteration {iteration.value}: \\n\\tAverage Return = {avg_return}\")\n",
    "\n",
    "            iteration.value += 1\n",
    "        finally:\n",
    "            lock.release()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    print(f\"\\nEnd of episode at iteration {iteration.value - 1}: \\n\\tEpisode reward = {total_reward}\")\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def multistep_advantage_actor_critic(\n",
    "        actor_critic: ActorCritic,\n",
    "        it: mp.Value,\n",
    "        gamma: float,\n",
    "        nb_steps: int,\n",
    "        max_iter: int,\n",
    "        lock: mp.Lock\n",
    "):\n",
    "    print(it.value)\n",
    "    env = gym.make('CartPole-v1')\n",
    "    while it.value <= max_iter:\n",
    "        print(it.value)\n",
    "        # Run one episode of A2C\n",
    "        total_reward = multistep_advantage_actor_critic_episode(\n",
    "            env=env,\n",
    "            actor_critic=actor_critic,\n",
    "            iteration=it,\n",
    "            gamma=gamma,\n",
    "            nb_steps=nb_steps,\n",
    "            max_iter=max_iter,\n",
    "            lock=lock\n",
    "        )\n",
    "\n",
    "\n",
    "def train_advantage_actor_critic(nb_actors: int = 1, nb_steps: int = 1, max_iter: int = 500000, gamma: int = 0.99):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    nb_states = env.observation_space.shape[0]\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    actor_critic = ActorCritic(nb_states, nb_actions)\n",
    "    it = mp.Value('i', 1)\n",
    "\n",
    "    lock = mp.Lock()\n",
    "    actor_critic.share_memory()\n",
    "    processes = []\n",
    "    for _ in range(nb_actors):\n",
    "        print(nb_actors)\n",
    "        process = mp.Process(\n",
    "            target=multistep_advantage_actor_critic,\n",
    "            args=(actor_critic, it, gamma, nb_steps, max_iter, lock)\n",
    "        )\n",
    "        process.start()\n",
    "        processes.append(process)\n",
    "        print(process)\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "\n",
    "def evaluate(actor_critic: ActorCritic, nb_episodes: int = 10):\n",
    "    env = gym.make('CartPole-v1')\n",
    "\n",
    "    episode_returns = []\n",
    "    plot_states = []\n",
    "    plot_values = []\n",
    "\n",
    "    for e in range(nb_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.Tensor(state)\n",
    "        done = False\n",
    "        undiscounted_return = 0\n",
    "\n",
    "        while not done:\n",
    "            action = actor_critic.take_best_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            undiscounted_return += reward\n",
    "            done = truncated or terminated\n",
    "\n",
    "            if e == nb_episodes - 1:\n",
    "                plot_states.append(state.detach())\n",
    "                plot_values.append(actor_critic.get_value(state).detach().item())\n",
    "\n",
    "            state = torch.Tensor(next_state)\n",
    "        episode_returns.append(undiscounted_return)\n",
    "\n",
    "    utils.plot_critic_values(np.array(plot_states), plot_values)\n",
    "\n",
    "    return np.mean(episode_returns)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T15:42:02.921719Z",
     "start_time": "2024-04-30T15:42:02.818124Z"
    }
   },
   "cell_type": "code",
   "source": "train_advantage_actor_critic()",
   "id": "5fcbffe8a6dea866",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T15:34:19.086557Z",
     "start_time": "2024-04-30T15:34:18.301145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ctypes\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "nb_states = env.observation_space.shape[0]\n",
    "nb_actions = env.action_space.n\n",
    "actor_critic = ActorCritic(nb_states, nb_actions)\n",
    "iteration = mp.Value(ctypes.c_int, 0)\n",
    "gamma = 0.99\n",
    "lock = mp.Lock()\n",
    "nb_steps = 1\n",
    "max_iter = 50000\n",
    "total_rewards = multistep_advantage_actor_critic_episode(\n",
    "    env, actor_critic, iteration, gamma, lock,nb_steps,max_iter\n",
    ")"
   ],
   "id": "550593d806d8a44e",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T15:33:41.513512Z",
     "start_time": "2024-04-30T15:33:41.494496Z"
    }
   },
   "cell_type": "code",
   "source": "print(total_rewards)",
   "id": "2050fc08824c452a",
   "execution_count": 4,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
