{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C Tutorial Notebook\n",
    "\n",
    "This notebook is here to guide you through the basics of the frameworks necessary for you to do well on your CS456 mini-project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:27.749367700Z",
     "start_time": "2024-04-16T15:50:25.678662400Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium environments\n",
    "\n",
    "One of the main and most spread environment frameworks in the field of RL research is [Gymnasium](https://gymnasium.farama.org/).\n",
    " It provides standardized environments offering a large range of difficulties and setups, that are well designed to benchmark performances of RL and Deep RL algorithms.\n",
    "\n",
    "The main structure is very simple to understand. First, we need to instantiate our environment. We will use an existing environment, but one could also use their structure to design their own environment.\n",
    "\n",
    "Let's directly work with the CartPole environment that will be used in the project. \n",
    "\n",
    "_PS: If you're more curious, feel free to browse the large list available on their website!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:27.841567600Z",
     "start_time": "2024-04-16T15:50:27.751362800Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains an action space and an observation (state) space. Let's see what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:27.936367100Z",
     "start_time": "2024-04-16T15:50:27.843073800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:28.013113700Z",
     "start_time": "2024-04-16T15:50:27.937364500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions available: 2\n",
      "Observation shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of actions available: {env.action_space.n}\")\n",
    "print(f\"Observation shape: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the action space of that first environment is discrete and contains 2 possible actions: push the cart left or right.\n",
    "\n",
    "The observation space has a dimension of 4, and you can find what each part represents [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space).\n",
    "\n",
    "Before taking actions, the environment should be reset (or boostrapped).\n",
    " **Note: this should be done every time the environment has to be restarted, i.e., at the end of any episode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:28.107012Z",
     "start_time": "2024-04-16T15:50:28.015123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: [-0.00777764 -0.04499613 -0.00859622  0.01105308]\n"
     ]
    }
   ],
   "source": [
    "# the second return value is an info dictionary, but it doesn't contain anything in this environment\n",
    "starting_state, _ = env.reset() \n",
    "\n",
    "print(f\"Starting state: {starting_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the actions look like and that the environment is ready, we can take actions inside it. This is done using the `env.step` function, that takes an action as input, and returns multiple values. More details on each of them can be found [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.step).\n",
    "\n",
    "In the project, you will have an agent that will choose an action (based on the policy learned) given the current state. However, for now, we can simply sample actions at random using `action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:28.200145600Z",
     "start_time": "2024-04-16T15:50:28.109974400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 1\n",
      "Next state: [-0.00867756  0.15024804 -0.00837516 -0.28432962]\n",
      "Reward: 1.0\n",
      "Terminated: False\n",
      "Truncated: False\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")\n",
    "next_state, reward, terminated, truncated, _ = env.step(action) # again, the last return value is an empty info object\n",
    "\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `terminated` and `truncated`  variables represent the two ways that the episode might be done.\n",
    "`terminated` indicates an MDP terminal state, and the reward will always be 0 afterward in case the horizon is longer or infinite.\n",
    "`truncated` indicates an artificial ending of the trajectory when the horizon may be infinite (to not run forever).\n",
    "Therefore, you should bootstrap the returns with the value function based on only `terminated`.\n",
    "However, you should use both to decide when to reset the environment:\n",
    "```\n",
    "done = terminated or truncated\n",
    "```\n",
    "\n",
    "We now have all the pieces necessary to run a full episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:28.295521400Z",
     "start_time": "2024-04-16T15:50:28.202172500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward after taking random actions: 11.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Episode reward after taking random actions: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your goal in the project will be to code an agent that can beat that."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T15:50:28.337022800Z",
     "start_time": "2024-04-16T15:50:28.296519500Z"
    }
   },
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
